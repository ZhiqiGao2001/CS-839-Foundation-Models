  0%|â–‰                                                                                                                                                                                                              | 199/45900 [00:53<3:23:24,  3.74it/s]Traceback (most recent call last):
{'loss': 10.7271, 'grad_norm': 3.2182517051696777, 'learning_rate': 4.9989106753812637e-05, 'epoch': 0.0}
{'loss': 10.5853, 'grad_norm': 1.5993576049804688, 'learning_rate': 4.997821350762528e-05, 'epoch': 0.0}
{'loss': 10.4969, 'grad_norm': 1.5362358093261719, 'learning_rate': 4.996732026143791e-05, 'epoch': 0.01}
{'loss': 10.4112, 'grad_norm': 1.5518560409545898, 'learning_rate': 4.9956427015250546e-05, 'epoch': 0.01}
{'loss': 10.3179, 'grad_norm': 1.3356964588165283, 'learning_rate': 4.994553376906319e-05, 'epoch': 0.01}
{'loss': 10.2135, 'grad_norm': 1.9135620594024658, 'learning_rate': 4.993464052287582e-05, 'epoch': 0.01}
{'loss': 10.1499, 'grad_norm': 2.918370485305786, 'learning_rate': 4.9923747276688455e-05, 'epoch': 0.02}
{'loss': 10.0568, 'grad_norm': 1.405705213546753, 'learning_rate': 4.9912854030501096e-05, 'epoch': 0.02}
{'loss': 10.0314, 'grad_norm': 1.2634813785552979, 'learning_rate': 4.990196078431373e-05, 'epoch': 0.02}
{'loss': 9.8685, 'grad_norm': 1.6244652271270752, 'learning_rate': 4.9891067538126364e-05, 'epoch': 0.02}
{'loss': 9.8286, 'grad_norm': 1.369258165359497, 'learning_rate': 4.9880174291939005e-05, 'epoch': 0.02}
{'loss': 9.7535, 'grad_norm': 1.957275390625, 'learning_rate': 4.986928104575164e-05, 'epoch': 0.03}
{'loss': 9.6797, 'grad_norm': 1.56376314163208, 'learning_rate': 4.9858387799564274e-05, 'epoch': 0.03}
{'loss': 9.5689, 'grad_norm': 1.3814892768859863, 'learning_rate': 4.984749455337691e-05, 'epoch': 0.03}
{'loss': 9.5495, 'grad_norm': 1.3244810104370117, 'learning_rate': 4.983660130718955e-05, 'epoch': 0.03}
{'loss': 9.482, 'grad_norm': 1.3356945514678955, 'learning_rate': 4.982570806100218e-05, 'epoch': 0.03}
{'loss': 9.4231, 'grad_norm': 1.4182192087173462, 'learning_rate': 4.981481481481482e-05, 'epoch': 0.04}
{'loss': 9.3301, 'grad_norm': 1.31560480594635, 'learning_rate': 4.980392156862745e-05, 'epoch': 0.04}
{'loss': 9.2655, 'grad_norm': 1.2518573999404907, 'learning_rate': 4.9793028322440085e-05, 'epoch': 0.04}
  File "/home/zhiqi/CS839-Project/small_model.py", line 88, in <module>
    normal_training(model, data_collator, train_dataset, eval_dataset, hps)
  File "/home/zhiqi/CS839-Project/small_model.py", line 75, in normal_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2486, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 88, in <module>
    normal_training(model, data_collator, train_dataset, eval_dataset, hps)
  File "/home/zhiqi/CS839-Project/small_model.py", line 75, in normal_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2486, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
