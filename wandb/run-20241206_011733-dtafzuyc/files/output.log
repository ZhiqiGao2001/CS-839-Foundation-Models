  2%|████▏                                                                                                                                                                                                              | 90/4590 [00:24<20:00,  3.75it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 10.7065, 'grad_norm': 3.55669903755188, 'learning_rate': 4.9891067538126364e-05, 'epoch': 0.0}
{'loss': 10.547, 'grad_norm': 1.6052255630493164, 'learning_rate': 4.9782135076252726e-05, 'epoch': 0.0}
{'loss': 10.4732, 'grad_norm': 1.4391499757766724, 'learning_rate': 4.967320261437909e-05, 'epoch': 0.01}
{'loss': 10.3912, 'grad_norm': 1.5649089813232422, 'learning_rate': 4.956427015250545e-05, 'epoch': 0.01}
{'loss': 10.2932, 'grad_norm': 1.3313312530517578, 'learning_rate': 4.945533769063181e-05, 'epoch': 0.01}
{'loss': 10.1925, 'grad_norm': 2.060154914855957, 'learning_rate': 4.9346405228758174e-05, 'epoch': 0.01}
{'loss': 10.1202, 'grad_norm': 3.223586320877075, 'learning_rate': 4.9237472766884536e-05, 'epoch': 0.02}
{'loss': 10.0344, 'grad_norm': 1.3104221820831299, 'learning_rate': 4.91285403050109e-05, 'epoch': 0.02}
{'loss': 10.0034, 'grad_norm': 1.2854359149932861, 'learning_rate': 4.901960784313725e-05, 'epoch': 0.02}
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 182, in prepare_dream_inputs
    'input_ids': torch.stack(input_ids, dim=0),
RuntimeError: stack expects each tensor to be equal size, but got [8, 4088] at entry 0 and [2048] at entry 5
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 182, in prepare_dream_inputs
    'input_ids': torch.stack(input_ids, dim=0),
RuntimeError: stack expects each tensor to be equal size, but got [8, 4088] at entry 0 and [2048] at entry 5
