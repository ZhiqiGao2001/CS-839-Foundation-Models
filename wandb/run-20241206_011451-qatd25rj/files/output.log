  2%|████▏                                                                                                                                                                                                              | 90/4590 [00:24<20:03,  3.74it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 10.717, 'grad_norm': 3.949277639389038, 'learning_rate': 4.9891067538126364e-05, 'epoch': 0.0}
{'loss': 10.5565, 'grad_norm': 1.6940604448318481, 'learning_rate': 4.9782135076252726e-05, 'epoch': 0.0}
{'loss': 10.4787, 'grad_norm': 1.5347654819488525, 'learning_rate': 4.967320261437909e-05, 'epoch': 0.01}
{'loss': 10.3794, 'grad_norm': 1.5437155961990356, 'learning_rate': 4.956427015250545e-05, 'epoch': 0.01}
{'loss': 10.2853, 'grad_norm': 1.354035496711731, 'learning_rate': 4.945533769063181e-05, 'epoch': 0.01}
{'loss': 10.1697, 'grad_norm': 2.0760650634765625, 'learning_rate': 4.9346405228758174e-05, 'epoch': 0.01}
{'loss': 10.1125, 'grad_norm': 3.6319010257720947, 'learning_rate': 4.9237472766884536e-05, 'epoch': 0.02}
{'loss': 10.0307, 'grad_norm': 1.2768659591674805, 'learning_rate': 4.91285403050109e-05, 'epoch': 0.02}
{'loss': 9.9942, 'grad_norm': 1.2504401206970215, 'learning_rate': 4.901960784313725e-05, 'epoch': 0.02}
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 142, in prepare_dream_inputs
    input_ids = torch.cat([item['input_ids'] for item in dream_data], dim=0)
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 49 for tensor number 5 in the list.
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 142, in prepare_dream_inputs
    input_ids = torch.cat([item['input_ids'] for item in dream_data], dim=0)
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 49 for tensor number 5 in the list.
