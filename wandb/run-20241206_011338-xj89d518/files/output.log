  2%|████▏                                                                                                                                                                                                              | 90/4590 [00:24<20:02,  3.74it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 10.7221, 'grad_norm': 3.433809757232666, 'learning_rate': 4.9891067538126364e-05, 'epoch': 0.0}
{'loss': 10.5749, 'grad_norm': 1.7125730514526367, 'learning_rate': 4.9782135076252726e-05, 'epoch': 0.0}
{'loss': 10.4837, 'grad_norm': 1.457471489906311, 'learning_rate': 4.967320261437909e-05, 'epoch': 0.01}
{'loss': 10.4008, 'grad_norm': 1.5513514280319214, 'learning_rate': 4.956427015250545e-05, 'epoch': 0.01}
{'loss': 10.3052, 'grad_norm': 1.3460694551467896, 'learning_rate': 4.945533769063181e-05, 'epoch': 0.01}
{'loss': 10.1976, 'grad_norm': 1.9914239645004272, 'learning_rate': 4.9346405228758174e-05, 'epoch': 0.01}
{'loss': 10.1262, 'grad_norm': 3.716059684753418, 'learning_rate': 4.9237472766884536e-05, 'epoch': 0.02}
{'loss': 10.0418, 'grad_norm': 1.3308258056640625, 'learning_rate': 4.91285403050109e-05, 'epoch': 0.02}
{'loss': 10.0195, 'grad_norm': 1.2307891845703125, 'learning_rate': 4.901960784313725e-05, 'epoch': 0.02}
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 142, in prepare_dream_inputs
    input_ids = torch.cat([item['input_ids'] for item in dream_data], dim=0)
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 49 for tensor number 5 in the list.
Traceback (most recent call last):
  File "/home/zhiqi/CS839-Project/small_model.py", line 114, in <module>
    initialize_training(
  File "/home/zhiqi/CS839-Project/small_model.py", line 89, in initialize_training
    trainer.train()
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/zhiqi/anaconda3/envs/llmmath/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 56, in training_step
    loss = self.dream_training_step(model, inputs)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 83, in dream_training_step
    dream_inputs = self.prepare_dream_inputs(dream_data)
  File "/home/zhiqi/CS839-Project/dream_trainer.py", line 142, in prepare_dream_inputs
    input_ids = torch.cat([item['input_ids'] for item in dream_data], dim=0)
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 2048 but got size 49 for tensor number 5 in the list.
